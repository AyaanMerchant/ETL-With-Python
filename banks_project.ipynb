{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Saylani Mass Training Program**\n",
    "### **Cloud Data Engineering  - Ayaan Merchant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A basice Extract, Transform and Load (ETL) pipeline using web scrapping, pandas and sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO  # Provides an in-memory file-like object for handling string data\n",
    "import requests  # Used to make HTTP requests to fetch web pages or APIs\n",
    "from bs4 import BeautifulSoup  # A library for parsing HTML and extracting data from it\n",
    "import pandas as pd  # A powerful library for data manipulation and analysis\n",
    "import sqlite3  # A library for SQLite database operations\n",
    "from datetime import datetime  # Provides classes for manipulating dates and times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0: Maintaining a Log File\n",
    "This step is done to record the logs while performing ETL and it is not neccessary in an ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    \"\"\"Log progress messages to a file.\"\"\"\n",
    "    \n",
    "    # Define the directory where log files will be stored\n",
    "    log_dir = 'logs'  # Folder named 'logs' will contain the log files\n",
    "    \n",
    "    # Create the directory if it doesn't already exist\n",
    "    os.makedirs(log_dir, exist_ok=True)  # `exist_ok=True` prevents errors if the folder already exists\n",
    "    \n",
    "    # Define the path for the log file within the directory\n",
    "    log_file_path = os.path.join(log_dir, 'code_log.txt')  # Log file named 'code_log.txt'\n",
    "    \n",
    "    # Open the log file in append mode to add new log messages\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        # Write the current timestamp and the provided message to the log file\n",
    "        f.write(f'{datetime.now()}: {message}\\n')  # Each log entry includes a timestamp and the message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url, table_attribs):\n",
    "    \"\"\" \n",
    "    This function extracts the required information from a website and saves it to a DataFrame.\n",
    "    The function returns the DataFrame for further processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch the HTML content of the webpage using the provided URL\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')  # Parse the HTML content using BeautifulSoup\n",
    "    \n",
    "    # Locate the specific table in the HTML using the given table attributes\n",
    "    table = soup.find('span', string=table_attribs).find_next('table')  \n",
    "    # Find a <span> tag with the specified string (table_attribs) and find the next <table> tag\n",
    "    \n",
    "    # Convert the extracted HTML table into a pandas DataFrame\n",
    "    df = pd.read_html(StringIO(str(table)))[0]  \n",
    "    # `pd.read_html` reads HTML tables directly into a DataFrame. The `StringIO` wraps the table's HTML string.\n",
    "\n",
    "    # Log progress to indicate successful data extraction and the start of transformation\n",
    "    log_progress('Data extraction complete. Initiating Transformation process')  \n",
    "    \n",
    "    # Return the extracted DataFrame for further processing\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, csv_path):\n",
    "    \"\"\" \n",
    "    This function processes the input DataFrame to add transformed versions of the Market Cap column.\n",
    "    It reads exchange rate information from a CSV file and creates three new columns with values\n",
    "    in different currencies (GBP, EUR, INR).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the exchange rate data from the CSV file and convert it to a dictionary\n",
    "    # The 'Rate' column values become accessible as dictionary values\n",
    "    exchange_rate = pd.read_csv(csv_path, index_col=0).to_dict()['Rate']  \n",
    "    \n",
    "    # Add a new column to the DataFrame for Market Cap in GBP, rounding to 2 decimal places\n",
    "    df['MC_GBP_Billion'] = round(df['Market cap (US$ billion)'] * exchange_rate['GBP'], 2)  \n",
    "    \n",
    "    # Add a new column to the DataFrame for Market Cap in EUR, rounding to 2 decimal places\n",
    "    df['MC_EUR_Billion'] = round(df['Market cap (US$ billion)'] * exchange_rate['EUR'], 2)  \n",
    "    \n",
    "    # Add a new column to the DataFrame for Market Cap in INR, rounding to 2 decimal places\n",
    "    df['MC_INR_Billion'] = round(df['Market cap (US$ billion)'] * exchange_rate['INR'], 2)  \n",
    "    \n",
    "    # Print the value of the 'MC_EUR_Billion' column for the 5th row to verify calculations\n",
    "    print(df['MC_EUR_Billion'][4])  \n",
    "    \n",
    "    # Log progress to indicate successful data transformation and the start of the loading process\n",
    "    log_progress('Data transformation complete. Initiating Loading process')  \n",
    "    \n",
    "    # Return the transformed DataFrame for further steps\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_csv(df, output_path):\n",
    "    \"\"\" \n",
    "    This function saves the final DataFrame as a CSV file at the specified path. \n",
    "    The function does not return anything.\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the DataFrame to a CSV file at the given output path\n",
    "    df.to_csv(output_path)  \n",
    "    \n",
    "    # Log progress to indicate that the data has been successfully saved to a CSV file\n",
    "    log_progress('Data saved to CSV file')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_db(df, sql_connection, table_name):\n",
    "    \"\"\" \n",
    "    This function saves the final DataFrame to a database table with the provided name.\n",
    "    The function does not return anything.\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the DataFrame to the database as a table with the specified name\n",
    "    # If the table already exists, replace it\n",
    "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)  \n",
    "    \n",
    "    # Log progress to indicate that the data has been loaded into the database\n",
    "    log_progress('Data loaded to Database as a table, Executing queries')  \n",
    "\n",
    "\n",
    "def run_query(query_statement, sql_connection):\n",
    "    \"\"\" \n",
    "    This function runs a query on the database table and prints the output on the terminal.\n",
    "    The function does not return anything.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a cursor object to interact with the database\n",
    "    cursor = sql_connection.cursor()  \n",
    "    \n",
    "    # Execute the provided SQL query\n",
    "    cursor.execute(query_statement)  \n",
    "    \n",
    "    # Fetch all results from the executed query\n",
    "    result = cursor.fetchall()  \n",
    "    \n",
    "    # Log progress to indicate the query execution is complete\n",
    "    log_progress('Process Complete')  \n",
    "    \n",
    "    # Return the query results for further use or inspection\n",
    "    return result  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted DataFrame:    Rank                                Bank name  Market cap (US$ billion)\n",
      "0     1                           JPMorgan Chase                    432.92\n",
      "1     2                          Bank of America                    231.52\n",
      "2     3  Industrial and Commercial Bank of China                    194.56\n",
      "3     4               Agricultural Bank of China                    160.68\n",
      "4     5                                HDFC Bank                    157.91\n",
      "153.17\n",
      "Transformed DataFrame:    Rank                                Bank name  Market cap (US$ billion)  \\\n",
      "0     1                           JPMorgan Chase                    432.92   \n",
      "1     2                          Bank of America                    231.52   \n",
      "2     3  Industrial and Commercial Bank of China                    194.56   \n",
      "3     4               Agricultural Bank of China                    160.68   \n",
      "4     5                                HDFC Bank                    157.91   \n",
      "\n",
      "   MC_GBP_Billion  MC_EUR_Billion  MC_INR_Billion  \n",
      "0          346.34          419.93        37122.89  \n",
      "1          185.22          224.57        19852.84  \n",
      "2          155.65          188.72        16683.52  \n",
      "3          128.54          155.86        13778.31  \n",
      "4          126.33          153.17        13540.78  \n",
      "Data saved to CSV: ./output/Largest_banks_data.csv\n",
      "Data loaded to database: ./output/Banks.db\n",
      "Query Results:\n",
      "[(1, 'JPMorgan Chase', 432.92, 346.34, 419.93, 37122.89), (2, 'Bank of America', 231.52, 185.22, 224.57, 19852.84), (3, 'Industrial and Commercial Bank of China', 194.56, 155.65, 188.72, 16683.52), (4, 'Agricultural Bank of China', 160.68, 128.54, 155.86, 13778.31), (5, 'HDFC Bank', 157.91, 126.33, 153.17, 13540.78), (6, 'Wells Fargo', 155.87, 124.7, 151.19, 13365.85), (7, 'HSBC Holdings PLC', 148.9, 119.12, 144.43, 12768.18), (8, 'Morgan Stanley', 140.83, 112.66, 136.61, 12076.17), (9, 'China Construction Bank', 139.82, 111.86, 135.63, 11989.56), (10, 'Bank of China', 136.81, 109.45, 132.71, 11731.46)]\n",
      "[(151.987,)]\n",
      "[('JPMorgan Chase',), ('Bank of America',), ('Industrial and Commercial Bank of China',), ('Agricultural Bank of China',), ('HDFC Bank',)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start of the script, executes only when the script is run directly\n",
    "    try:\n",
    "        # File paths and constants\n",
    "        url = 'https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks'  \n",
    "        # URL for the web page to scrape data from\n",
    "        \n",
    "        output_csv_path = './output/Largest_banks_data.csv'  \n",
    "        # Path for saving the output CSV file\n",
    "        \n",
    "        database_name = './output/Banks.db'  \n",
    "        # Name and location of the SQLite database file\n",
    "        \n",
    "        table_name = 'Largest_banks'  \n",
    "        # Name of the database table to store the data\n",
    "\n",
    "        # Ensure the output directory exists, creating it if necessary\n",
    "        os.makedirs('./output', exist_ok=True)  \n",
    "\n",
    "        # Log progress indicating that preliminary steps are complete\n",
    "        log_progress('Preliminaries complete. Initiating ETL process')  \n",
    "\n",
    "        # Step 1: Extract data\n",
    "        df = extract(url, 'By market capitalization')  \n",
    "        # Extract data from the specified URL and table identifier\n",
    "\n",
    "        if df is None:\n",
    "            # Check if the extract function returned a valid DataFrame\n",
    "            raise ValueError(\"Extract function returned None\")  # Raise an error if no data was extracted\n",
    "        \n",
    "        print(\"Extracted DataFrame:\", df.head())  # Print the first few rows for debugging purposes\n",
    "\n",
    "        # Step 2: Transform the data\n",
    "        transform(df, './input/exchange_rate.csv')  \n",
    "        # Apply transformations using exchange rates from the specified CSV file\n",
    "\n",
    "        print(\"Transformed DataFrame:\", df.head())  # Print the first few rows after transformation for debugging\n",
    "\n",
    "        # Step 3: Load transformed data into a CSV file\n",
    "        load_to_csv(df, output_csv_path)  \n",
    "        # Save the transformed data to a CSV file at the specified path\n",
    "\n",
    "        print(f\"Data saved to CSV: {output_csv_path}\")  # Confirm the CSV save operation\n",
    "\n",
    "        # Step 4: Load transformed data into a database\n",
    "        with sqlite3.connect(database_name) as conn:  \n",
    "            # Establish a connection to the SQLite database (context manager ensures connection is closed)\n",
    "            load_to_db(df, conn, table_name)  \n",
    "            # Load the DataFrame into a database table\n",
    "\n",
    "            print(f\"Data loaded to database: {database_name}\")  # Confirm the database load operation\n",
    "\n",
    "            # Step 5: Run queries on the database\n",
    "            print(\"Query Results:\")\n",
    "            print(run_query('SELECT * FROM Largest_banks', conn))  \n",
    "            # Retrieve all rows from the database table\n",
    "            \n",
    "            print(run_query('SELECT AVG(MC_GBP_Billion) FROM Largest_banks', conn))  \n",
    "            # Calculate and display the average market cap in GBP\n",
    "            \n",
    "            print(run_query('SELECT \"Bank name\" FROM Largest_banks LIMIT 5', conn))  \n",
    "            # Retrieve the first 5 bank names\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the ETL process\n",
    "        log_progress(f\"Error occurred: {str(e)}\")  \n",
    "        # Log the error message\n",
    "\n",
    "        print(f\"Error occurred: {str(e)}\")  # Print the error message to the console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
